{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Certificate Big Data - Centrale Supelec - Octobre 2018\n",
    "# Détection d’intrusion réseau à l’aide de l’apprentissage automatique\n",
    "### Notebook : sp5-Spark-ml-classification\n",
    "### Auteur : Ahmed Mekaouar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark running in jupyter notebook in local mode (pyspark --master local[2])\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building IDS model using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load & Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve Cleaned/processed data ready -- data from \"processed\" directory\n",
    "baseline_file = os.path.join(os.path.pardir,'data','train_baseline.csv')\n",
    "attacks_file = os.path.join(os.path.pardir,'data','train_attacks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_baseline = spark.read.csv(baseline_file,header=True,sep=\",\",inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_attacks = spark.read.csv(attacks_file,header=True,sep=\",\",inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all=df_baseline.union(df_attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_attacks.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#COLUMNS = (\"TotalBackwardPackets\", \"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_attacks.select(*COLUMNS).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_attacks.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_all.columns[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#col=df_all.columns[0:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename Label column -> stringLabel - spark is not case sensitive regarding columns names!\n",
    "df_all=df_all.withColumnRenamed(\"Label\", \"stringLabel\")\n",
    "# To be directly done on files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Init_Win_bytes_backward: int, Init_Win_bytes_forward: int, FwdPacketLengthMax: double, min_seg_size_forward: int, PacketLengthMean: double, SubflowFwdBytes: int, BwdPacketLengthMean: double, BwdPacketLengthMax: double, AvgBwdSegmentSize: double, BwdPackets/s: double, FlowIATMean: double, BwdPacketLengthMin: double, FlowIATMin: double, BwdHeaderLength: int, FwdIATMin: double, TotalLengthofBwdPackets: double, FlowPackets/s: double, TotalLengthofFwdPackets: double, SubflowBwdBytes: int, FwdIATMean: double, AveragePacketSize: double, FlowDuration: int, MaxPacketLength: double, FlowIATStd: double, FwdIATTotal: double, FwdPacketLengthMean: double, FlowBytesPs: double, PacketLengthVariance: double, FwdIATStd: double, FlowIATMax: double, FwdIATMax: double, PacketLengthStd: double, FwdPackets/s: double, FwdHeaderLength: int, SubflowFwdPackets: int, AvgFwdSegmentSize: double, SubflowBwdPackets: int, act_data_pkt_fwd: int, FwdPacketLengthStd: double, TotalFwdPackets: int, TotalBackwardPackets: int, stringLabel: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MLLib require Label to be numerical so we will use StringIndexer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "labelIndexer = StringIndexer().setInputCol(\"stringLabel\").setOutputCol(\"label\") .fit(df_all) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#labelIndexer.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexed_df_all = labelIndexer.transform(df_all) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ft_col=indexed_df_all.columns[0:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#indexed_df_all.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VectorAssembler concatenate all your features into one big vector called \"features\"\n",
    "# This is mondatoty \n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler().setInputCols(indexed_df_all.columns[0:-2]).setOutputCol(\"features\")\n",
    "data_ml = vectorAssembler.transform(indexed_df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_ml.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_ml.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data set : train 70% , test 30%\n",
    "data_ml_train, df_ml_test = data_ml.randomSplit([0.7,0.3],seed=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1761879"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ml_train.count()\n",
    "# data_ml_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_ml_train.groupBy(\"label\").count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Init_Win_bytes_backward: int, Init_Win_bytes_forward: int, FwdPacketLengthMax: double, min_seg_size_forward: int, PacketLengthMean: double, SubflowFwdBytes: int, BwdPacketLengthMean: double, BwdPacketLengthMax: double, AvgBwdSegmentSize: double, BwdPackets/s: double, FlowIATMean: double, BwdPacketLengthMin: double, FlowIATMin: double, BwdHeaderLength: int, FwdIATMin: double, TotalLengthofBwdPackets: double, FlowPackets/s: double, TotalLengthofFwdPackets: double, SubflowBwdBytes: int, FwdIATMean: double, AveragePacketSize: double, FlowDuration: int, MaxPacketLength: double, FlowIATStd: double, FwdIATTotal: double, FwdPacketLengthMean: double, FlowBytesPs: double, PacketLengthVariance: double, FwdIATStd: double, FlowIATMax: double, FwdIATMax: double, PacketLengthStd: double, FwdPackets/s: double, FwdHeaderLength: int, SubflowFwdPackets: int, AvgFwdSegmentSize: double, SubflowBwdPackets: int, act_data_pkt_fwd: int, FwdPacketLengthStd: double, TotalFwdPackets: int, TotalBackwardPackets: int, stringLabel: string, label: double, features: vector]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ml_train.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "decisiontree =  DecisionTreeClassifier(labelCol=\"label\",featuresCol=\"features\",seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 ms, sys: 0 ns, total: 32 ms\n",
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dt_model = decisiontree.fit(data_ml_train)\n",
    "# fitting the Decision Tree classificatio model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict label for test data\n",
    "y_test_pred = dt_model.transform(df_ml_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Init_Win_bytes_backward: int, Init_Win_bytes_forward: int, FwdPacketLengthMax: double, min_seg_size_forward: int, PacketLengthMean: double, SubflowFwdBytes: int, BwdPacketLengthMean: double, BwdPacketLengthMax: double, AvgBwdSegmentSize: double, BwdPackets/s: double, FlowIATMean: double, BwdPacketLengthMin: double, FlowIATMin: double, BwdHeaderLength: int, FwdIATMin: double, TotalLengthofBwdPackets: double, FlowPackets/s: double, TotalLengthofFwdPackets: double, SubflowBwdBytes: int, FwdIATMean: double, AveragePacketSize: double, FlowDuration: int, MaxPacketLength: double, FlowIATStd: double, FwdIATTotal: double, FwdPacketLengthMean: double, FlowBytesPs: double, PacketLengthVariance: double, FwdIATStd: double, FlowIATMax: double, FwdIATMax: double, PacketLengthStd: double, FwdPackets/s: double, FwdHeaderLength: int, SubflowFwdPackets: int, AvgFwdSegmentSize: double, SubflowBwdPackets: int, act_data_pkt_fwd: int, FwdPacketLengthStd: double, TotalFwdPackets: int, TotalBackwardPackets: int, stringLabel: string, label: double, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# persisting y_test_pred\n",
    "y_test_pred.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_test_pred.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "model_evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_metrics(y_test_pred):\n",
    "    print('Accuracy : %4f'  %model_evaluator.evaluate(y_test_pred, {model_evaluator.metricName: \"accuracy\"}) )\n",
    "    print('Precision : %4f'  %model_evaluator.evaluate(y_test_pred, {model_evaluator.metricName: \"weightedPrecision\"}) )\n",
    "    print('Recall : %4f'  %model_evaluator.evaluate(y_test_pred, {model_evaluator.metricName: \"weightedRecall\"}) )\n",
    "    print('F1 : %4f'  %model_evaluator.evaluate(y_test_pred, {model_evaluator.metricName: \"f1\"}) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.959411\n",
      "Precision : 0.954285\n",
      "Recall : 0.959411\n",
      "F1 : 0.952071\n"
     ]
    }
   ],
   "source": [
    "model_metrics(y_test_pred)\n",
    "# Results are the equivalent of scikit learn weighted metrics.\n",
    "# Spark model performance is slightly worse than scikit learn one. The reason is that I did not perform any class balacing\n",
    "# effort. The model is doing very well anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and fitting a  pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trying to keep the code simple, I did not include majority class undersampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a pipeline with all transformation + model\n",
    "ids_pipeline = Pipeline().setStages([labelIndexer, vectorAssembler, decisiontree])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all_train, df_all_test = df_all.randomSplit([0.7,0.3],seed=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48 ms, sys: 4 ms, total: 52 ms\n",
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ids_model = ids_pipeline.fit(df_all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = ids_model.transform(df_all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Init_Win_bytes_backward: int, Init_Win_bytes_forward: int, FwdPacketLengthMax: double, min_seg_size_forward: int, PacketLengthMean: double, SubflowFwdBytes: int, BwdPacketLengthMean: double, BwdPacketLengthMax: double, AvgBwdSegmentSize: double, BwdPackets/s: double, FlowIATMean: double, BwdPacketLengthMin: double, FlowIATMin: double, BwdHeaderLength: int, FwdIATMin: double, TotalLengthofBwdPackets: double, FlowPackets/s: double, TotalLengthofFwdPackets: double, SubflowBwdBytes: int, FwdIATMean: double, AveragePacketSize: double, FlowDuration: int, MaxPacketLength: double, FlowIATStd: double, FwdIATTotal: double, FwdPacketLengthMean: double, FlowBytesPs: double, PacketLengthVariance: double, FwdIATStd: double, FlowIATMax: double, FwdIATMax: double, PacketLengthStd: double, FwdPackets/s: double, FwdHeaderLength: int, SubflowFwdPackets: int, AvgFwdSegmentSize: double, SubflowBwdPackets: int, act_data_pkt_fwd: int, FwdPacketLengthStd: double, TotalFwdPackets: int, TotalBackwardPackets: int, stringLabel: string, label: double, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.959411\n",
      "Precision : 0.954285\n",
      "Recall : 0.959411\n",
      "F1 : 0.952071\n"
     ]
    }
   ],
   "source": [
    "model_metrics(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the pipeline and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# persisting the pipeline\n",
    "pipeline_path_file = os.path.join(os.path.pardir,'models','ids_pipeline')\n",
    "ids_pipeline.write().overwrite().save(pipeline_path_file)\n",
    "# persisting the model\n",
    "model_path_file = os.path.join(os.path.pardir,'models','ids_ml_model')\n",
    "ids_model.write().overwrite().save(model_path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_pipeline = Pipeline.load(pipeline_path_file)\n",
    "#loaded_ids_model = PipelineModel.load(model_path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
